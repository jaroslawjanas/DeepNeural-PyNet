{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import math\n",
    "import sys\n",
    "from IPython.display import Latex"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1 - Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I took a very modular approach to building my models. As you can see below I give the user an option to specify their own functions if they want to. I don't use it much in the first few models but will showcase it more for part 5.\n",
    "\n",
    "I didn't really bother getting everything perfectly here, the model doesn't really check for convergence and just runs as long as it's told to.\n",
    "Weights are a 1D array and bias is just a constant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class LogisticReg():\n",
    "\n",
    "    def __init__(self):\n",
    "        self.input_size = None\n",
    "        self.weights = None\n",
    "        self.bias = None\n",
    "        self.activation = None\n",
    "\n",
    "    @staticmethod\n",
    "    def weights_init(size):\n",
    "        return np.random.uniform(-1/100, 1/100, size)\n",
    "\n",
    "    @staticmethod\n",
    "    def bias_init():\n",
    "        return np.random.uniform(-1/100, 1/100, 1)\n",
    "\n",
    "    @staticmethod\n",
    "    def sigmoid(x):\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "\n",
    "    @staticmethod\n",
    "    def logistic_loss(a, y):\n",
    "        return - (y * math.log(a) + (1 - y) * math.log(1 - a))\n",
    "\n",
    "    def train(self, data, labels, alpha=0.1, epochs=1,\n",
    "              weights_init = None,\n",
    "              biases_init = None,\n",
    "              activation = None,\n",
    "             ):\n",
    "\n",
    "        if weights_init is None:\n",
    "            weights_init = LogisticReg.weights_init\n",
    "\n",
    "        if biases_init is None:\n",
    "            biases_init = LogisticReg.bias_init\n",
    "\n",
    "        if activation is None:\n",
    "            self.activation = LogisticReg.sigmoid\n",
    "\n",
    "        self.input_size = np.shape(data[0])[0]\n",
    "        self.weights = weights_init(self.input_size)\n",
    "        self.bias = biases_init()\n",
    "\n",
    "        for _ in range(epochs):\n",
    "            for dp, label in zip(data, labels):\n",
    "                a = self.predict(dp)\n",
    "\n",
    "                # Taken from notes\n",
    "                dL_dw = (a - label) * dp\n",
    "                dL_db = a - label\n",
    "\n",
    "                self.weights -= alpha * dL_dw\n",
    "                self.bias -= alpha * dL_db\n",
    "\n",
    "    def predict(self, in_dp):\n",
    "        z = np.sum(in_dp * self.weights) + self.bias\n",
    "        return float(self.activation(z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "lr = LogisticReg()\n",
    "lr.train(np.array([[1, 1, 1], [0, 0 ,0], [1, 1, 1], [1, 1, 1], [0, 0, 0]]), np.array([1, 0, 1, 1, 0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2 - LR Easy Tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "import sklearn.datasets\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Display plots inline and change default figure size\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "## Based on the provided code\n",
    "\n",
    "# Use pandas to read the CSV file as a dataframe\n",
    "df_blobs = pd.read_csv(\"blobs300.csv\")\n",
    "\n",
    "# The y values are those labelled 'Class': extract their values\n",
    "classes_blobs = df_blobs['Class'].values\n",
    "\n",
    "# The x values are all other columns\n",
    "del df_blobs['Class']   # drop the 'Class' column from the dataframe\n",
    "data_blobs = df_blobs.values    # convert the remaining columns to a numpy array\n",
    "\n",
    "\n",
    "# Use pandas to read the CSV file as a dataframe\n",
    "df_circles = pd.read_csv(\"circles600.csv\")\n",
    "\n",
    "# The y values are those labelled 'Class': extract their values\n",
    "classes_circles = df_circles['Class'].values\n",
    "\n",
    "# The x values are all other columns\n",
    "del df_circles['Class']     # drop the 'Class' column from the dataframe\n",
    "data_circles = df_circles.values    # convert the remaining columns to a numpy array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shuffle and Split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Shuffles provided data, splits it according to the provided parameters and returns a dictionary with `[\"training]`, `[\"validation\"]` and `[\"testing\"]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def shuffle_and_split(data, labels, validation_split=0.7, test_split=0.15):\n",
    "    # Below 3 lines based on https://play.pixelblaster.ro/blog/2017/01/20/how-to-shuffle-two-arrays-to-the-same-order/\n",
    "    l = labels.shape[0]\n",
    "    s = np.arange(l) # seed\n",
    "    np.random.shuffle(s)\n",
    "    #-----------\n",
    "\n",
    "    data = np.array(data[s])\n",
    "    labels = np.array(labels[s])\n",
    "\n",
    "    validation_pivot = int(l * validation_split)\n",
    "    testing_pivot = int(validation_pivot + l * test_split)\n",
    "\n",
    "    data = {\n",
    "        \"training\" : {\n",
    "            \"input\": data[0 : validation_pivot],\n",
    "            \"classes\": labels[0 : validation_pivot]\n",
    "        },\n",
    "        \"validation\" : {\n",
    "            \"input\": data[validation_pivot : testing_pivot],\n",
    "            \"classes\": labels[validation_pivot : testing_pivot],\n",
    "        },\n",
    "        \"testing\" : {\n",
    "            \"input\": data[testing_pivot : l],\n",
    "            \"classes\": labels[testing_pivot : l]\n",
    "        },\n",
    "    }\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train - Blobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "data = dict()\n",
    "data[\"blobs\"] = shuffle_and_split(data_blobs, classes_blobs, validation_split=0.7, test_split=0.15)\n",
    "\n",
    "lr_blob = LogisticReg()\n",
    "lr_blob.train(data[\"blobs\"][\"training\"][\"input\"], data[\"blobs\"][\"training\"][\"classes\"], alpha = 0.03)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict - Blobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 0.9777777777777777\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "for dp, label in zip(data[\"blobs\"][\"validation\"][\"input\"], data[\"blobs\"][\"validation\"][\"classes\"]):\n",
    "    a = lr_blob.predict(dp)\n",
    "\n",
    "    if round(a) == label:\n",
    "        correct += 1\n",
    "\n",
    "print(f\"Accuracy = {correct / len(data['blobs']['validation']['classes']) }\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test - Blobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 0.9555555555555556\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "for dp, label in zip(data[\"blobs\"][\"testing\"][\"input\"], data[\"blobs\"][\"testing\"][\"classes\"]):\n",
    "    a = lr_blob.predict(dp)\n",
    "\n",
    "    if round(a) == label:\n",
    "        correct += 1\n",
    "\n",
    "print(f\"Accuracy = {correct / len(data['blobs']['testing']['classes']) }\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "As expected the results for linearly separable data are very good."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train - Circles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "data[\"circles\"] = shuffle_and_split(data_circles, classes_circles, validation_split=0.7, test_split=0.15)\n",
    "\n",
    "lr_circles = LogisticReg()\n",
    "lr_circles.train(data[\"circles\"][\"training\"][\"input\"], data[\"circles\"][\"training\"][\"classes\"], alpha=0.05)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict - Circles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 0.6222222222222222\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "for dp, label in zip(data[\"circles\"][\"validation\"][\"input\"], data[\"circles\"][\"validation\"][\"classes\"]):\n",
    "    a = lr_circles.predict(dp)\n",
    "\n",
    "    if round(a) == label:\n",
    "        correct += 1\n",
    "\n",
    "print(f\"Accuracy = {correct / len(data['circles']['validation']['classes'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing - Circles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 0.6777777777777778\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "for dp, label in zip(data[\"circles\"][\"testing\"][\"input\"], data[\"circles\"][\"testing\"][\"classes\"]):\n",
    "    a = lr_circles.predict(dp)\n",
    "\n",
    "    if round(a) == label:\n",
    "        correct += 1\n",
    "\n",
    "print(f\"Accuracy = {correct / len(data['circles']['testing']['classes'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Logistic regression is not a good fit for non-linearly separable data, hence the poor (random) result."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3 - Shallow Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class ShallowNetwork:\n",
    "    epsilon = sys.float_info.epsilon\n",
    "\n",
    "    def __init__(self):\n",
    "        self.input_size = None\n",
    "        self.weights = None\n",
    "        self.biases = None\n",
    "        self.activation = None\n",
    "        self.activation_derivative = None\n",
    "        self.loss_function = None\n",
    "        self.normalizer = None\n",
    "\n",
    "    @staticmethod\n",
    "    def weights_init(size):\n",
    "        return np.random.normal(0, 0.2, size)\n",
    "        # Based on  xavier initialization\n",
    "        # https://machinelearningmastery.com/weight-initialization-for-deep-learning-neural-networks/\n",
    "        # n = 1\n",
    "        # if len(size) > 1:\n",
    "        #     n = size[1]\n",
    "        # return np.random.uniform(-1/math.sqrt(n), 1/math.sqrt(n), size)\n",
    "\n",
    "    @staticmethod\n",
    "    def biases_init(size):\n",
    "        return np.random.normal(0, 0.2, size)\n",
    "\n",
    "    # Because of overflow\n",
    "    # as explained here\n",
    "    # https://shaktiwadekar.medium.com/how-to-avoid-numerical-overflow-in-sigmoid-function-numerically-stable-sigmoid-function-5298b14720f6\n",
    "    @staticmethod\n",
    "    def sigmoid(x):\n",
    "        return np.where( x >= 0, (1 / (1 + np.exp(-x))), (np.exp(x) / (1 + np.exp(x))) )\n",
    "\n",
    "    @staticmethod\n",
    "    def sigmoid_derivative(x):\n",
    "        return ShallowNetwork.sigmoid(x) * (1 - ShallowNetwork.sigmoid(x))\n",
    "\n",
    "    @staticmethod\n",
    "    def logistic_loss(a, y):\n",
    "        return - (y * math.log(a + ShallowNetwork.epsilon) + (1 - y) * math.log(1 - a + ShallowNetwork.epsilon))\n",
    "\n",
    "    # Proper transpose, not this numpy BS\n",
    "    @staticmethod\n",
    "    def transpose(x):\n",
    "        if len(np.shape(x)) == 1:\n",
    "            return x.reshape(1, -1)\n",
    "        return np.transpose(x)\n",
    "\n",
    "    # Based on https://en.wikipedia.org/wiki/Feature_scaling\n",
    "    @staticmethod\n",
    "    def normalize(x, x_min, x_max, new_min=-1, new_max=1):\n",
    "        return ((x - x_min) / (x_max - x_min)) * (new_max - new_min) + new_min\n",
    "\n",
    "    @staticmethod\n",
    "    def normalizer_constructor(data, normalize):\n",
    "        t_data = ShallowNetwork.transpose(data)\n",
    "        x_min = []\n",
    "        x_max = []\n",
    "        for d_vector in t_data:\n",
    "            x_min.append(min(d_vector))\n",
    "            x_max.append(max(d_vector))\n",
    "\n",
    "        return lambda x: np.array([normalize(xx, x_min[idx], x_max[idx]) for idx, xx in enumerate(x)])\n",
    "\n",
    "    def train(self, data, labels, k=10, alpha=0.5, stop_early=False, stop_avg_loss=0.01, n_loss_avg = 10,\n",
    "              iterations=None,\n",
    "              weights_init = None,\n",
    "              biases_init = None,\n",
    "              activation = None,\n",
    "              activation_derivative = None,\n",
    "              loss_function = None,\n",
    "              normalize = None\n",
    "             ):\n",
    "\n",
    "        if iterations is None:\n",
    "            iterations = len(data)\n",
    "\n",
    "        if weights_init is None:\n",
    "            weights_init = ShallowNetwork.weights_init\n",
    "\n",
    "        if biases_init is None:\n",
    "            biases_init = ShallowNetwork.biases_init\n",
    "\n",
    "        if activation is None:\n",
    "            activation = ShallowNetwork.sigmoid\n",
    "\n",
    "        if activation_derivative is None:\n",
    "            activation_derivative = ShallowNetwork.sigmoid_derivative\n",
    "\n",
    "        if loss_function is None:\n",
    "            loss_function = ShallowNetwork.logistic_loss\n",
    "\n",
    "        if normalize is None:\n",
    "            normalize = ShallowNetwork.normalize\n",
    "\n",
    "        self.input_size = np.shape(data[0])[0]\n",
    "        self.weights = [weights_init((k, self.input_size)), weights_init((1, k))]\n",
    "        self.biases = [biases_init(k), biases_init(1)]\n",
    "        self.activation = np.vectorize(activation)\n",
    "        self.activation_derivative = np.vectorize(activation_derivative)\n",
    "        self.loss_function = np.vectorize(loss_function)\n",
    "        alpha = float(alpha)\n",
    "        self.normalizer = ShallowNetwork.normalizer_constructor(data, normalize)\n",
    "\n",
    "        losses = []\n",
    "        for iter in range(iterations):\n",
    "            [dp, label] = random.choice(list((zip(data, labels))))\n",
    "\n",
    "            a, z = self.f_propagate(dp)\n",
    "            if stop_early:\n",
    "                loss = self.loss_function(a[-1], label)[0]\n",
    "                if len(losses) == n_loss_avg:\n",
    "                    avg_loss = sum(losses) / len(losses)\n",
    "                    if avg_loss < stop_avg_loss:\n",
    "                        print(f\"Stopped after {iter} iterations\")\n",
    "                        break\n",
    "                losses.append(loss)\n",
    "                losses = losses[-n_loss_avg:]\n",
    "\n",
    "            # Based on my old assignment https://github.com/jaroslawjanas/NN-Perceptron\n",
    "            # (math is included in the repo https://github.com/jaroslawjanas/NN-Perceptron/tree/master/maths) which\n",
    "            # is based on this playlist of videos https://youtube.com/playlist?list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi\n",
    "            # and partially based on https://sudeepraja.github.io/Neural/\n",
    "\n",
    "            # Output Layer\n",
    "            dL_dz = a[-1] - label\n",
    "            dz_dw = a[-2]  # previous layer\n",
    "            dz_db = 1\n",
    "            dz_da_m1 = self.weights[-1]\n",
    "\n",
    "\n",
    "            dL_dw = dL_dz.dot(ShallowNetwork.transpose(dz_dw))\n",
    "            dL_db = dL_dz * dz_db\n",
    "            dL_da_m1 = ShallowNetwork.transpose(dz_da_m1).dot(dL_dz) # this line is based on https://sudeepraja.github.io/Neural/\n",
    "\n",
    "            self.weights[-1] -= alpha * dL_dw\n",
    "            self.biases[-1] -= alpha * dL_db\n",
    "\n",
    "            # Hidden Layer\n",
    "            da_m1_dz_m1 = activation_derivative(z[-2])\n",
    "            dL_dz_m1 = dL_da_m1 * da_m1_dz_m1\n",
    "            dz_m1_dw_m1 = a[-3]\n",
    "            dz_m1_db_m1 = 1\n",
    "\n",
    "            dL_dw_m1 = dL_dz_m1[:,np.newaxis].dot(ShallowNetwork.transpose(dz_m1_dw_m1))\n",
    "            # above line is based on https://sudeepraja.github.io/Neural/\n",
    "            dL_db_m1 = dL_dz_m1 * dz_m1_db_m1\n",
    "\n",
    "            self.weights[-2] -= alpha * dL_dw_m1\n",
    "            self.biases[-2] -= alpha * dL_db_m1\n",
    "\n",
    "    def f_propagate(self, in_dp):\n",
    "        norm_in_dp = self.normalizer(in_dp)\n",
    "        a = [norm_in_dp]\n",
    "        zz = []\n",
    "        for weights, biases in zip(self.weights, self.biases):\n",
    "\n",
    "            z = np.dot(weights, a[-1]) + biases\n",
    "            zz = zz + [z]\n",
    "            a = a + [self.activation(z)]\n",
    "\n",
    "        return a, zz\n",
    "\n",
    "    def predict(self, in_dp):\n",
    "        a, _ = self.f_propagate(in_dp)\n",
    "        return a[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Blobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopped after 421 iterations\n"
     ]
    }
   ],
   "source": [
    "snn_blobs = ShallowNetwork()\n",
    "snn_blobs.train(data[\"blobs\"][\"training\"][\"input\"], data[\"blobs\"][\"training\"][\"classes\"],\n",
    "                k=3 , alpha=0.5, iterations=50_000,\n",
    "                stop_early=True, stop_avg_loss=0.01, n_loss_avg=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 1.0\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "for dp, label in zip(data[\"blobs\"][\"testing\"][\"input\"], data[\"blobs\"][\"testing\"][\"classes\"]):\n",
    "    a = snn_blobs.predict(dp)\n",
    "\n",
    "    if round(a[0]) == label:\n",
    "        correct += 1\n",
    "\n",
    "print(f\"Accuracy = {correct / len(data['blobs']['testing']['classes'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 1.0\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "for dp, label in zip(data[\"blobs\"][\"validation\"][\"input\"], data[\"blobs\"][\"validation\"][\"classes\"]):\n",
    "    a = snn_blobs.predict(dp)\n",
    "\n",
    "    if round(a[0]) == label:\n",
    "        correct += 1\n",
    "\n",
    "print(f\"Accuracy = {correct / len(data['blobs']['validation']['classes'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Circles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopped after 38475 iterations\n"
     ]
    }
   ],
   "source": [
    "snn_circles = ShallowNetwork()\n",
    "snn_circles.train(data[\"circles\"][\"training\"][\"input\"], data[\"circles\"][\"training\"][\"classes\"],\n",
    "                  k=3, alpha = 0.1, iterations = 50_000,\n",
    "                  stop_early=True, stop_avg_loss=0.005, n_loss_avg=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 1.0\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "for dp, label in zip(data[\"circles\"][\"testing\"][\"input\"], data[\"circles\"][\"testing\"][\"classes\"]):\n",
    "    a = snn_circles.predict(dp)\n",
    "\n",
    "    if round(a[0]) == label:\n",
    "        correct += 1\n",
    "\n",
    "print(f\"Accuracy = {correct / len(data['circles']['testing']['classes'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 1.0\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "for dp, label in zip(data[\"circles\"][\"validation\"][\"input\"], data[\"circles\"][\"validation\"][\"classes\"]):\n",
    "    a = snn_circles.predict(dp)\n",
    "\n",
    "    if round(a[0]) == label:\n",
    "        correct += 1\n",
    "\n",
    "print(f\"Accuracy = {correct / len(data['circles']['validation']['classes'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopped after 26599 iterations\n",
      "Stopped after 37494 iterations\n",
      "Stopped after 43035 iterations\n",
      "Stopped after 39425 iterations\n",
      "Stopped after 28735 iterations\n",
      "Stopped after 48262 iterations\n",
      "Stopped after 31017 iterations\n",
      "Stopped after 26566 iterations\n",
      "Stopped after 26189 iterations\n",
      "Avg accuracy 0.891\n"
     ]
    }
   ],
   "source": [
    "acc = 0\n",
    "for i in range(10):\n",
    "    snn_circles = ShallowNetwork()\n",
    "    snn_circles.train(data[\"circles\"][\"training\"][\"input\"], data[\"circles\"][\"training\"][\"classes\"],\n",
    "                  k=3, alpha = 0.1, iterations = 50_000,\n",
    "                  stop_early=True, stop_avg_loss=0.005, n_loss_avg=5)\n",
    "\n",
    "    correct = 0\n",
    "    for dp, label in zip(data[\"circles\"][\"testing\"][\"input\"], data[\"circles\"][\"testing\"][\"classes\"]):\n",
    "        a = snn_circles.predict(dp)\n",
    "\n",
    "        if round(a[0]) == label:\n",
    "            correct += 1\n",
    "\n",
    "    acc += round(correct / len(data['circles']['testing']['classes']), 2)\n",
    "\n",
    "print(f\"Avg accuracy {acc/10}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Part 4 - Difficult Task "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Literally just copied from the provided example\n",
    "\n",
    "def unpickle(file):\n",
    "    import pickle\n",
    "    with open(file, 'rb') as fo:\n",
    "        dict = pickle.load(fo, encoding='bytes')\n",
    "    return dict\n",
    "\n",
    "def loadbatch(batchname):\n",
    "    folder = 'cifar-10-batches-py'\n",
    "    batch = unpickle(folder+\"/\"+batchname)\n",
    "    return batch\n",
    "\n",
    "def loadlabelnames():\n",
    "    folder = 'cifar-10-batches-py'\n",
    "    meta = unpickle(folder+\"/\"+'batches.meta')\n",
    "    return meta[b'label_names']\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def visualise(data, index):\n",
    "    # MM Jan 2019: Given a CIFAR data nparray and the index of an image, display the image.\n",
    "    # Note that the images will be quite fuzzy looking, because they are low res (32x32).\n",
    "\n",
    "    picture = data[index]\n",
    "    # Initially, the data is a 1D array of 3072 pixels; reshape it to a 3D array of 3x32x32 pixels\n",
    "    # Note: after reshaping like this, you could select one colour channel or average them.\n",
    "    picture.shape = (3,32,32)\n",
    "\n",
    "    # Plot.imshow requires the RGB to be the third dimension, not the first, so need to rearrange\n",
    "    picture = picture.transpose([1, 2, 0])\n",
    "    plt.imshow(picture)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Loading a batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of items in the batch is 4\n",
      "All keys in the batch: dict_keys([b'batch_label', b'labels', b'data', b'filenames'])\n"
     ]
    }
   ],
   "source": [
    "batch1 = loadbatch('data_batch_1')\n",
    "print(\"Number of items in the batch is\", len(batch1))\n",
    "\n",
    "# Display all keys, so we can see the ones we want\n",
    "print('All keys in the batch:', batch1.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Separating the data into classes and showing sample images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size of data in this batch: 10000 , size of labels: 10000\n",
      "<class 'numpy.ndarray'>\n",
      "(10000, 3072)\n",
      "Horse idx shape: (1001,)\n",
      "Truck data shape: (1001, 3, 32, 32)\n",
      "Truck idx shape: (981,)\n",
      "Truck data shape: (981, 3, 32, 32)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "cf_data = batch1[b'data']\n",
    "cf_labels = batch1[b'labels']\n",
    "\n",
    "print (\"size of data in this batch:\", len(cf_data), \", size of labels:\", len(cf_labels))\n",
    "print (type(cf_data))\n",
    "print(cf_data.shape)\n",
    "\n",
    "# names = loadlabelnames()\n",
    "# print(names)\n",
    "\n",
    "# My modifications\n",
    "horse_idxs = np.where(np.array(cf_labels) == 7)[0]\n",
    "horse_data = np.array(cf_data[horse_idxs])\n",
    "horse_data.shape = (len(horse_idxs), 3,32,32)\n",
    "print(f\"Horse idx shape: {np.shape(horse_idxs)}\")\n",
    "print(f\"Truck data shape: {np.shape(horse_data)}\")\n",
    "\n",
    "truck_idxs = np.where(np.array(cf_labels) ==9)[0]\n",
    "truck_data = np.array(cf_data[truck_idxs])\n",
    "truck_data.shape = (len(truck_idxs), 3,32,32)\n",
    "\n",
    "print(f\"Truck idx shape: {np.shape(truck_idxs)}\")\n",
    "print(f\"Truck data shape: {np.shape(truck_data)}\")\n",
    "\n",
    "picture = horse_data[0]\n",
    "# picture.shape = (3,32,32)\n",
    "\n",
    "picture = picture.transpose([1, 2, 0])\n",
    "plt.imshow(picture)\n",
    "plt.show()\n",
    "\n",
    "picture = truck_data[0]\n",
    "# picture.shape = (3,32,32)\n",
    "\n",
    "picture = picture.transpose([1, 2, 0])\n",
    "plt.imshow(picture)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Joining the two classes into a single dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "H&T = (1001, 3, 32, 32)\n",
      "+     (981, 3, 32, 32)\n",
      "H&T = (1982, 3, 32, 32)\n",
      "\n",
      "H&T_c = (1001,)\n",
      "+       (981,)\n",
      "H&T_c = (1982,)\n"
     ]
    }
   ],
   "source": [
    "horse_truck_data = np.array(horse_data)\n",
    "print(f\"H&T = {np.shape(horse_truck_data)}\")\n",
    "\n",
    "print(f\"+     {np.shape(truck_data)}\")\n",
    "\n",
    "horse_truck_data = np.append(horse_truck_data, truck_data, axis=0)\n",
    "print(f\"H&T = {np.shape(horse_truck_data)}\\n\")\n",
    "\n",
    "horse_truck_classes = np.array(np.zeros(len(horse_data)))\n",
    "print(f\"H&T_c = {np.shape(horse_truck_classes)}\")\n",
    "\n",
    "print(f\"+       {np.shape(np.zeros(len(truck_data)))}\")\n",
    "horse_truck_classes = np.append(horse_truck_classes, np.ones(len(truck_data)))\n",
    "print(f\"H&T_c = {np.shape(horse_truck_classes)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Converting the RGB images to grayscale and showing examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1982, 32, 32)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "horse_truck_data_grayscale = np.sum(horse_truck_data, axis=1) / 3\n",
    "print(np.shape(horse_truck_data_grayscale))\n",
    "\n",
    "plt.imshow(horse_truck_data_grayscale[0], cmap='gray')\n",
    "plt.show()\n",
    "\n",
    "plt.imshow(horse_truck_data_grayscale[1001], cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1982, 1024)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "horse_truck_data_grayscale.shape = (len(horse_truck_data_grayscale), 1024)\n",
    "np.shape(horse_truck_data_grayscale)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Splitting the data into `[\"training\"]`, `[\"validation\"]` and `[\"testing\"]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "data[\"horse_truck\"] = shuffle_and_split(horse_truck_data_grayscale, horse_truck_classes, validation_split=0.7, test_split=0.15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Training a shallow net on the `horse_truck` data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopped after 5421 iterations\n"
     ]
    }
   ],
   "source": [
    "snn_horse_truck = ShallowNetwork()\n",
    "snn_horse_truck.train(data[\"horse_truck\"][\"training\"][\"input\"], data[\"horse_truck\"][\"training\"][\"classes\"],\n",
    "                  k=20, alpha=0.1, iterations=30_000,\n",
    "                  stop_early=True, stop_avg_loss=0.01, n_loss_avg=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Testing the shallow net."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 0.7583892617449665\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "for dp, label in zip(data[\"horse_truck\"][\"testing\"][\"input\"], data[\"horse_truck\"][\"testing\"][\"classes\"]):\n",
    "    a = snn_horse_truck.predict(dp)\n",
    "\n",
    "    if round(a[0]) == label:\n",
    "        correct += 1\n",
    "\n",
    "print(f\"Accuracy = {correct / len(data['horse_truck']['testing']['classes'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 0.7609427609427609\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "for dp, label in zip(data[\"horse_truck\"][\"validation\"][\"input\"], data[\"horse_truck\"][\"validation\"][\"classes\"]):\n",
    "    a = snn_horse_truck.predict(dp)\n",
    "\n",
    "    if round(a[0]) == label:\n",
    "        correct += 1\n",
    "\n",
    "print(f\"Accuracy = {correct / len(data['horse_truck']['validation']['classes'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "It performs very well. Even better than the deep neural network without any improvements."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Part 5 - Mathematics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Self made image from my old assignment [github](https://github.com/jaroslawjanas/NN-Perceptron/tree/master/maths)\n",
    "which was based on [3blue1brown](https://youtube.com/playlist?list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi) neural networks playlist\n",
    "![DNN Tree](https://i.imgur.com/euAGKgQ.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "\\begin{array}\n",
    "\\\\\n",
    "\\text{-------- Functions --------}\n",
    "\\\\\n",
    "C_{0} = - (y \\ln (a) + (1-y) \\ln (1-a))\n",
    "\\\\\n",
    "a^{(L)} = \\sigma (z^{(L)})\n",
    "\\\\\n",
    "z^{(L)} = w^{(L)} a^{(L-1)} + b\n",
    "\\\\\n",
    "\\sigma(z) = \\frac{1}{1 + e^{-x}}\n",
    "\\end{array}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{array}\n",
    "\\\\\n",
    "\\text{-------- Derivatives of Functions --------}\n",
    "\\\\\n",
    "\\frac{\\partial C_{0}}{\\partial a^{(L)}} = \\frac{a - y}{a(1-a)}\n",
    "\\\\\n",
    "\\frac{\\partial a^{(L)}}{\\partial z^{(L)}} = \\sigma \\prime (z^{(L)})\n",
    "\\\\\n",
    "\\sigma \\prime = \\frac{e^{-z}}{(1 + e^{-z})^{2}}\n",
    "\\\\\n",
    "\\frac{\\partial C_{0}}{\\partial z^{(L)}} = \\frac{\\partial C_{0}}{\\partial a^{(L)}} \\frac{\\partial a^{(L)}}{\\partial z^{(L)}}\n",
    "\\\\\n",
    "\\frac{\\partial C_{0}}{\\partial z^{(L)}} = a^{(L)} - y\n",
    "\\\\\n",
    "\\frac{\\partial z^{(L)}}{\\partial w^{(L)}} = a^{(L-1)}\n",
    "\\\\\n",
    "\\frac{\\partial z^{(L)}}{\\partial a^{(L-1)}} = w^{(L)}\n",
    "\\\\\n",
    "\\frac{\\partial z^{(L)}}{\\partial b^{(L)}} = 1\n",
    "\\\\\n",
    "\\sigma(z) = \\frac{1}{1 + e^{-x}}\n",
    "\\end{array}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{array}\n",
    "\\\\\n",
    "\\text{-------- Output Layer --------}\\\\\n",
    "\\\\\n",
    "\\frac{\\partial C_{0}}{\\partial w^{(L)}} =\\ \\frac{\\partial C_{0}}{\\partial a^{(L)}} \\ \\frac{\\partial a^{(L)}}{\\partial z^{(L)}} \\ \\frac{\\partial z^{(L)}}{\\partial w^{(L)}}\\\\\n",
    "\\\\\n",
    "\\frac{\\partial C_{0}}{\\partial b^{(L)}} \\ =\\frac{\\partial C_{0}}{\\partial a^{(L)}} \\ \\frac{\\partial a^{(L)}}{\\partial z^{(L)}} \\ \\frac{\\partial z^{(L)}}{\\partial b^{(L)}}\\\\\n",
    "\\\\\n",
    "\\frac{\\partial C_{0}}{\\partial a^{(L-1)}} =\\frac{\\partial C_{0}}{\\partial a^{(L)}} \\ \\frac{\\partial a^{(L)}}{\\partial z^{(L)}} \\ \\frac{\\partial z^{(L)}}{\\partial a^{(L-1)}}\\\\\n",
    "\\\\\n",
    "\\text{Note the common element}\\\\\n",
    "\\\\\n",
    "\\frac{\\partial C_{0}}{\\partial z^{(L)}} \\ =\\frac{\\partial C_{0}}{\\partial a^{(L)}} \\ \\frac{\\partial a^{(L)}}{\\partial z^{(L)}}\\\\\n",
    "\\\\\n",
    "\\text{Hence}\\\\\n",
    "\\\\\n",
    "\\frac{\\partial C_{0}}{\\partial w^{(L)}} =\\ \\frac{\\partial C_{0}}{\\partial z^{(L)}} \\ \\frac{\\partial z^{(L)}}{\\partial w^{(L)}}\\\\\n",
    "\\\\\n",
    "\\frac{\\partial C_{0}}{\\partial b^{(L)}} \\ =\\frac{\\partial C_{0}}{\\partial z^{(L)}} \\ \\frac{\\partial z^{(L)}}{\\partial b^{(L)}}\\\\\n",
    "\\\\\n",
    "\\frac{\\partial C_{0}}{\\partial a^{(L-1)}} =\\frac{\\partial C_{0}}{\\partial z^{(L)}} \\ \\frac{\\partial z^{(L)}}{\\partial a^{(L-1)}}\\\\\n",
    "\\\\\n",
    "\\text{Substituting for derivatives from \"Derivatives of Functions\"}\\\\\n",
    "\\\\\n",
    "\\frac{\\partial C_{0}}{\\partial z^{(L)}} = a^{(L)} - y\n",
    "\\\\\n",
    "\\frac{\\partial C_{0}}{\\partial w^{(L)}} \\ =\\ \\frac{\\partial C_{0}}{\\partial z^{(L)}} \\ a^{(L-1)}\\\\\n",
    "\\\\\n",
    "\\frac{\\partial C_{0}}{\\partial b^{(L)}} =\\frac{\\partial C_{0}}{\\partial z^{(L)}} \\ 1\\\\\n",
    "\\\\\n",
    "\\frac{\\partial C_{0}}{\\partial a^{(L-1)}} =\\frac{\\partial C_{0}}{\\partial z^{(L)}} \\ w^{(L)}\\\\\n",
    "\\\\\n",
    "\\end{array}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "\\begin{array}\n",
    "\\\\\n",
    "\\text{-------- Hidden Layers --------}\\\\\n",
    "\\\\\n",
    "\\frac{\\partial C_{0}}{\\partial w^{(L-1)}} =\\ \\frac{\\partial C_{0}}{\\partial a^{(L-1)}} \\ \\frac{\\partial a^{(L-1)}}{\\partial z^{(L-1)}} \\ \\frac{\\partial z^{(L-1)}}{\\partial w^{(L-1)}}\\\\\n",
    "\\\\\n",
    "\\frac{\\partial C_{0}}{\\partial b^{(L-1)}} =\\ \\frac{\\partial C_{0}}{\\partial a^{(L-1)}} \\ \\frac{\\partial a^{(L-1)}}{\\partial z^{(L-1)}} \\ \\frac{\\partial z^{(L-1)}}{\\partial b^{(L-1)}}\\\\\n",
    "\\\\\n",
    "\\frac{\\partial C_{0}}{\\partial w^{(L-2)}} =\\ \\frac{\\partial C_{0}}{\\partial a^{(L-1)}} \\ \\frac{\\partial a^{(L-1)}}{\\partial z^{(L-1)}} \\ \\frac{\\partial z^{(L-1)}}{\\partial w^{(L-2)}}\\\\\n",
    "\\\\\n",
    "\\text{Note the common element}\\\\\n",
    "\\\\\n",
    "\\frac{\\partial C_{0}}{\\partial z^{(L-1)}} =\\frac{\\partial C_{0}}{\\partial a^{(L-1)}} \\ \\frac{\\partial a^{(L-1)}}{\\partial z^{(L-1)}}\\\\\n",
    "\\\\\n",
    "\\text{Hence}\\\\\n",
    "\\\\\n",
    "\\frac{\\partial C_{0}}{\\partial w^{(L-1)}} =\\ \\frac{\\partial C_{0}}{\\partial z^{(L-1)}} \\ \\frac{\\partial z^{(L-1)}}{\\partial w^{(L-1)}}\\\\\n",
    "\\\\\n",
    "\\frac{\\partial C_{0}}{\\partial b^{(L-1)}} =\\ \\frac{\\partial C_{0}}{\\partial z^{(L-1)}} \\ \\frac{\\partial z^{(L-1)}}{\\partial b^{(L-1)}}\\\\\n",
    "\\\\\n",
    "\\frac{\\partial C_{0}}{\\partial w^{(L-2)}} =\\ \\frac{\\partial C_{0}}{\\partial z^{(L-1)}} \\ \\frac{\\partial z^{(L-1)}}{\\partial w^{(L-2)}}\\\\\n",
    "\\\\\n",
    "\\text{Substituting for derivatives from \"Derivatives of Functions\"}\\\\\n",
    "\\\\\n",
    "\\frac{\\partial C_{0}}{\\partial z^{(L-1)}} =\\ \\frac{\\partial C_{0}}{\\partial a^{(L-1)}} \\ \\sigma \\prime (z^{(L-1)})\\\\\n",
    "\\\\\n",
    "\\frac{\\partial C_{0}}{\\partial w^{(L-1)}} =\\ \\frac{\\partial C_{0}}{\\partial z^{(L-1)}} \\ a^{(L-2)}\\\\\n",
    "\\\\\n",
    "\\frac{\\partial C_{0}}{\\partial b^{(L-1)}} =\\ \\frac{\\partial C_{0}}{\\partial z^{(L-1)}} \\ 1\\\\\n",
    "\\\\\n",
    "\\frac{\\partial C_{0}}{\\partial w^{(L-2)}} =\\ \\frac{\\partial C_{0}}{\\partial z^{(L-1)}} \\ w^{(L-1)}\\\\\n",
    "\\end{array}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Part 5 - Deep Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I tried to make my DNN fairly modular. That's why some functions can be passed as inputs.\n",
    "I had to derive `dL/da` (the loss with respect to activation) myself in order to allow for custom activation function in the output layer.\n",
    "I am not including the mathematics behind `dL/da` of the loss function because it's long.\n",
    "\n",
    "Additionally, some transposes used in the back-propagation were taken from [here](https://sudeepraja.github.io/Neural/).\n",
    "\n",
    "I had some problems with sigmoid `np.exp(-x)` causing overflow so I separated it into two functions as recommended [here](https://shaktiwadekar.medium.com/how-to-avoid-numerical-overflow-in-sigmoid-function-numerically-stable-sigmoid-function-5298b14720f6). Vectorizing it was quite difficult.\n",
    "Ah yes, and I did vectorize forward propagation.\n",
    "\n",
    "Note that normalize operates with respect to particular input, different normalizer will be created for `x` and `y` in `[[x0, y0], [x1, y1], [x2, y2]]`.\n",
    "\n",
    "Also at this point I also decided to implement early stopping which I later also put in the ShallowNetwork implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class DeepNetwork:\n",
    "    epsilon = sys.float_info.epsilon\n",
    "\n",
    "    def __init__(self):\n",
    "        self.input_size = []\n",
    "        self.k = []\n",
    "        self.weights = []\n",
    "        self.biases = []\n",
    "        self.activation = []\n",
    "        self.activation_derivative = []\n",
    "        self.loss_function = None\n",
    "        self.normalizer = None\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def weights_init(size):\n",
    "        # Based on xavier initialization\n",
    "        # https://machinelearningmastery.com/weight-initialization-for-deep-learning-neural-networks/\n",
    "        # n = 1\n",
    "        # if len(size) > 1:\n",
    "        #     n = size[1]\n",
    "        # return np.random.uniform(-1/math.sqrt(n), 1/math.sqrt(n), size)\n",
    "        return np.random.normal(0, 0.2, size)\n",
    "\n",
    "    @staticmethod\n",
    "    def biases_init(size):\n",
    "        return np.random.normal(0, 0.2, size)\n",
    "\n",
    "    # Because of overflow\n",
    "    # as explained here\n",
    "    # https://shaktiwadekar.medium.com/how-to-avoid-numerical-overflow-in-sigmoid-function-numerically-stable-sigmoid-function-5298b14720f6\n",
    "    @staticmethod\n",
    "    def sigmoid(x):\n",
    "        return np.where(x >= 0, (1 / (1 + np.exp(-x))), (np.exp(x) / (1 + np.exp(x))) )\n",
    "         # return 1 / (1 + np.exp(-x))\n",
    "\n",
    "    @staticmethod\n",
    "    def sigmoid_derivative(x):\n",
    "        return DeepNetwork.sigmoid(x) * (1 - DeepNetwork.sigmoid(x))\n",
    "\n",
    "    @staticmethod\n",
    "    def relu(x):\n",
    "        return max(0, x)\n",
    "\n",
    "    # Taken from https://stackoverflow.com/questions/46411180/implement-relu-derivative-in-python-numpy\n",
    "    @staticmethod\n",
    "    def relu_derivative(x):\n",
    "        return (x > 0) * 1\n",
    "\n",
    "    @staticmethod\n",
    "    def logistic_loss(a, y):\n",
    "        return - (y * math.log(a + DeepNetwork.epsilon) + (1 - y) * math.log(1 - a + DeepNetwork.epsilon))\n",
    "\n",
    "    # Proper transpose, not this numpy BS\n",
    "    @staticmethod\n",
    "    def transpose(x):\n",
    "        if len(np.shape(x)) == 1:\n",
    "            return x.reshape(1, -1)\n",
    "        return np.transpose(x)\n",
    "\n",
    "    @staticmethod\n",
    "    def one_d_always_two_d(x):\n",
    "        if len(np.shape(x)) == 1:\n",
    "            return x[:, np.newaxis]\n",
    "        else:\n",
    "            return x\n",
    "\n",
    "    # Based on https://en.wikipedia.org/wiki/Feature_scaling\n",
    "    @staticmethod\n",
    "    def normalize(x, x_min, x_max, new_min=-1, new_max=1):\n",
    "        return ((x - x_min) / (x_max - x_min)) * (new_max - new_min) + new_min\n",
    "\n",
    "    @staticmethod\n",
    "    def normalizer_constructor (data, normalize):\n",
    "        t_data = DeepNetwork.transpose(data)\n",
    "        x_min = []\n",
    "        x_max = []\n",
    "        for d_vector in t_data:\n",
    "            x_min.append(min(d_vector))\n",
    "            x_max.append(max(d_vector))\n",
    "\n",
    "        return lambda x: np.array([normalize(xx, x_min[idx], x_max[idx]) for idx, xx in enumerate(x)])\n",
    "\n",
    "\n",
    "    def add_layer(self, k, weights_init = None,\n",
    "                 biases_init = None,\n",
    "                 activation = None,\n",
    "                 activation_derivative = None,\n",
    "                 input_size = None\n",
    "                 ):\n",
    "\n",
    "        if weights_init is None:\n",
    "            weights_init = DeepNetwork.weights_init\n",
    "\n",
    "        if biases_init is None:\n",
    "            biases_init = DeepNetwork.biases_init\n",
    "\n",
    "        if activation is None:\n",
    "            activation = DeepNetwork.sigmoid\n",
    "\n",
    "        if activation_derivative is None:\n",
    "            activation_derivative = DeepNetwork.sigmoid_derivative\n",
    "\n",
    "        if len(self.input_size) == 0 and input_size is None:\n",
    "            raise Exception(\"You are creating an input layer and did not provide input size\")\n",
    "        elif len(self.k) > 0:\n",
    "            input_size = self.k[-1]\n",
    "\n",
    "        self.k.append(k)\n",
    "        self.input_size.append(input_size)\n",
    "        self.weights.append(weights_init((k, input_size)))\n",
    "        self.biases.append(biases_init((k,)))\n",
    "        self.activation.append(np.vectorize(activation))\n",
    "        self.activation_derivative.append(np.vectorize(activation_derivative))\n",
    "\n",
    "    def train(self, data, labels, alpha=0.01, stop_early=False, stop_avg_loss=0.01, n_loss_avg=10,\n",
    "              iterations=None,\n",
    "              loss_function=None,\n",
    "              normalize=None\n",
    "              ):\n",
    "\n",
    "        if iterations is None:\n",
    "            iterations = len(data)\n",
    "\n",
    "        if loss_function is None:\n",
    "            loss_function = DeepNetwork.logistic_loss\n",
    "\n",
    "        if normalize is None:\n",
    "            normalize = DeepNetwork.normalize\n",
    "\n",
    "        self.loss_function = np.vectorize(loss_function)\n",
    "        alpha = float(alpha)\n",
    "        self.normalizer = DeepNetwork.normalizer_constructor(data, normalize)\n",
    "\n",
    "        losses = []\n",
    "        for iter in range(iterations):\n",
    "            [dp, label] = random.choice(list((zip(data, labels))))\n",
    "\n",
    "            a, z = self.f_propagate(dp)\n",
    "            if stop_early:\n",
    "                loss = self.loss_function(a[-1], label)[0]\n",
    "                if len(losses) == n_loss_avg:\n",
    "                    avg_loss = sum(losses) / len(losses)\n",
    "                    if avg_loss < stop_avg_loss:\n",
    "                        print(f\"Stopped after {iter} iterations\")\n",
    "                        break\n",
    "                losses.append(loss)\n",
    "                losses = losses[-n_loss_avg:]\n",
    "\n",
    "            # Based on my old assignment https://github.com/jaroslawjanas/NN-Perceptron\n",
    "            # (math is included in the repo https://github.com/jaroslawjanas/NN-Perceptron/tree/master/maths) which\n",
    "            # is based on this playlist of videos https://youtube.com/playlist?list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi\n",
    "            # and partially based on https://sudeepraja.github.io/Neural/\n",
    "\n",
    "            # I also finally bothered to do the derivatives\n",
    "\n",
    "            # Was going to build a loop for this but I have to build one for part 5 anyway\n",
    "            # So I am not going to bother\n",
    "\n",
    "            dL_da_mn = None\n",
    "            for i in range(1, len(self.k)):\n",
    "\n",
    "                if(i == 1):\n",
    "                    # Output layer\n",
    "\n",
    "                    # dL_dz = a[-i] - label  # <-- This is faster because it's mathematically simplified\n",
    "                    # dL_da = (a[-i] - label) / ((a[-i] * (1 - a[-i])) + DeepNetwork.epsilon) # this is ugly\n",
    "\n",
    "                    # this avoids division by 0\n",
    "                    if (a[-i] * (1 - a[-i])) == 0:\n",
    "                        dL_da = (a[-i] - label) / DeepNetwork.epsilon\n",
    "                    else:\n",
    "                        dL_da = (a[-i] - label) / (a[-i] * (1 - a[-i]))\n",
    "\n",
    "                    da_dz = self.activation_derivative[-i](z[-i])\n",
    "                    dL_dz = dL_da *  da_dz # <-- but this is more flexible, I can specify my own activation for output layer\n",
    "                    dz_dw = a[-(i+1)]  # previous layer\n",
    "                    dz_db = 1\n",
    "                    dz_da_m1 = self.weights[-i]\n",
    "\n",
    "\n",
    "                    dL_dw = dL_dz.dot(DeepNetwork.transpose(dz_dw))\n",
    "                    dL_db = dL_dz * dz_db\n",
    "                    dL_da_mn = DeepNetwork.transpose(dz_da_m1).dot(dL_dz) # this line is based on https://sudeepraja.github.io/Neural/\n",
    "\n",
    "                    self.weights[-i] -= alpha * dL_dw\n",
    "                    self.biases[-i] -= alpha * dL_db\n",
    "\n",
    "                else:\n",
    "                    # Hidden Layers\n",
    "\n",
    "                    da_m1_dz_m1 = self.activation_derivative[-i](z[-i])\n",
    "                    dL_dz_m1 = dL_da_mn * da_m1_dz_m1\n",
    "                    dz_m1_dw_m1 = a[-(i+1)]\n",
    "                    dz_m1_db_m1 = 1\n",
    "                    if i is not len(self.k):\n",
    "                        dz_m1_da_m2 = self.weights[-i]\n",
    "\n",
    "                    # dL_dw_m1 = dL_dz_m1[:,np.newaxis].dot(DeepNetwork.transpose(dz_m1_dw_m1))\n",
    "                    dL_dw_m1 = DeepNetwork.one_d_always_two_d(dL_dz_m1).dot(DeepNetwork.transpose(dz_m1_dw_m1))\n",
    "                    # above line is based on https://sudeepraja.github.io/Neural/\n",
    "                    dL_db_m1 = dL_dz_m1 * dz_m1_db_m1\n",
    "\n",
    "                    if i is not len(self.k):\n",
    "                        dL_da_mn = DeepNetwork.transpose(dz_m1_da_m2).dot(dL_dz_m1)\n",
    "\n",
    "                    self.weights[-i] -= alpha * dL_dw_m1\n",
    "                    self.biases[-i] -= alpha * dL_db_m1\n",
    "\n",
    "    def f_propagate(self, in_dp):\n",
    "        norm_in_dp = self.normalizer(in_dp)\n",
    "        a = [norm_in_dp]\n",
    "        zz = []\n",
    "        for weights, biases, activation in zip(self.weights, self.biases, self.activation):\n",
    "\n",
    "            z = np.dot(weights, a[-1]) + biases\n",
    "            zz = zz + [z]\n",
    "            a = a + [activation(z)]\n",
    "\n",
    "        return a, zz\n",
    "\n",
    "    def predict(self, in_dp):\n",
    "        a, _ = self.f_propagate(in_dp)\n",
    "        return a[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Blobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopped after 15727 iterations\n"
     ]
    }
   ],
   "source": [
    "dnn_blobs = DeepNetwork()\n",
    "dnn_blobs.add_layer(10, input_size=len(data[\"blobs\"][\"training\"][\"input\"][0]), activation=DeepNetwork.relu, activation_derivative=DeepNetwork.relu_derivative)\n",
    "dnn_blobs.add_layer(1)\n",
    "\n",
    "dnn_blobs.train(data[\"blobs\"][\"training\"][\"input\"], data[\"blobs\"][\"training\"][\"classes\"],\n",
    "                alpha=0.05, iterations=50_000,\n",
    "                stop_early=True, stop_avg_loss=0.005, n_loss_avg=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 0.9333333333333333\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "for dp, label in zip(data[\"blobs\"][\"testing\"][\"input\"], data[\"blobs\"][\"testing\"][\"classes\"]):\n",
    "    a = dnn_blobs.predict(dp)\n",
    "\n",
    "    if round(a[0]) == label:\n",
    "        correct += 1\n",
    "\n",
    "print(f\"Accuracy = {correct / len(data['blobs']['testing']['classes'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 0.9555555555555556\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "for dp, label in zip(data[\"blobs\"][\"validation\"][\"input\"], data[\"blobs\"][\"validation\"][\"classes\"]):\n",
    "    a = dnn_blobs.predict(dp)\n",
    "\n",
    "    if round(a[0]) == label:\n",
    "        correct += 1\n",
    "\n",
    "print(f\"Accuracy = {correct / len(data['blobs']['validation']['classes'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Circles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopped after 3684 iterations\n"
     ]
    }
   ],
   "source": [
    "dnn_circles = DeepNetwork()\n",
    "dnn_circles.add_layer(32, input_size=len(data[\"circles\"][\"training\"][\"input\"][0]), activation=DeepNetwork.relu, activation_derivative=DeepNetwork.relu_derivative)\n",
    "dnn_circles.add_layer(16)\n",
    "dnn_circles.add_layer(1)\n",
    "\n",
    "dnn_circles.train(data[\"circles\"][\"training\"][\"input\"], data[\"circles\"][\"training\"][\"classes\"],\n",
    "                  alpha=0.15, iterations=20_000,\n",
    "                  stop_early=True, stop_avg_loss=0.005, n_loss_avg=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 1.0\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "for dp, label in zip(data[\"circles\"][\"testing\"][\"input\"], data[\"circles\"][\"testing\"][\"classes\"]):\n",
    "    a = dnn_circles.predict(dp)\n",
    "\n",
    "    if round(a[0]) == label:\n",
    "        correct += 1\n",
    "\n",
    "print(f\"Accuracy = {correct / len(data['circles']['testing']['classes'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 1.0\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "for dp, label in zip(data[\"circles\"][\"validation\"][\"input\"], data[\"circles\"][\"validation\"][\"classes\"]):\n",
    "    a = dnn_circles.predict(dp)\n",
    "\n",
    "    if round(a[0]) == label:\n",
    "        correct += 1\n",
    "\n",
    "print(f\"Accuracy = {correct / len(data['circles']['validation']['classes'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Horse Truck"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopped after 22143 iterations\n"
     ]
    }
   ],
   "source": [
    "dnn_horse_truck = DeepNetwork()\n",
    "dnn_horse_truck.add_layer(256, input_size=len(data[\"horse_truck\"][\"training\"][\"input\"][0]), activation=DeepNetwork.relu, activation_derivative=DeepNetwork.relu_derivative)\n",
    "dnn_horse_truck.add_layer(64, activation=DeepNetwork.relu, activation_derivative=DeepNetwork.relu_derivative)\n",
    "dnn_horse_truck.add_layer(32)\n",
    "dnn_horse_truck.add_layer(1)\n",
    "\n",
    "dnn_horse_truck.train(data[\"horse_truck\"][\"training\"][\"input\"], data[\"horse_truck\"][\"training\"][\"classes\"],\n",
    "                  # alpha=0.05, iterations=40_000,\n",
    "                  alpha=0.005, iterations=40_000,\n",
    "                  stop_early=True, stop_avg_loss=0.005, n_loss_avg=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 0.7651006711409396\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "for dp, label in zip(data[\"horse_truck\"][\"testing\"][\"input\"], data[\"horse_truck\"][\"testing\"][\"classes\"]):\n",
    "    a = dnn_horse_truck.predict(dp)\n",
    "\n",
    "    if round(a[0]) == label:\n",
    "        correct += 1\n",
    "\n",
    "print(f\"Accuracy = {correct / len(data['horse_truck']['testing']['classes'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 0.7676767676767676\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "for dp, label in zip(data[\"horse_truck\"][\"validation\"][\"input\"], data[\"horse_truck\"][\"validation\"][\"classes\"]):\n",
    "    a = dnn_horse_truck.predict(dp)\n",
    "\n",
    "    if round(a[0]) == label:\n",
    "        correct += 1\n",
    "\n",
    "print(f\"Accuracy = {correct / len(data['horse_truck']['validation']['classes'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Part 5 - DNN Mini batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Mini-batch changes weights after each mini-batch rather than each case.\n",
    "The advantage of this is that it can be parallelized (which can't be done easily on python unfortunately).\n",
    "\n",
    "Another advantage is that it only updates the weights every `n/mbatch_size` times and as `n` increases in sized the performance benefits become more visible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class MiniBatchDeepNetwork:\n",
    "    epsilon = sys.float_info.epsilon\n",
    "\n",
    "    def __init__(self):\n",
    "        self.input_size = []\n",
    "        self.k = []\n",
    "        self.weights = []\n",
    "        self.biases = []\n",
    "        self.activation = []\n",
    "        self.activation_derivative = []\n",
    "        self.loss_function = None\n",
    "        self.normalizer = None\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def weights_init(size):\n",
    "        # Based on xavier initialization\n",
    "        # https://machinelearningmastery.com/weight-initialization-for-deep-learning-neural-networks/\n",
    "        # n = 1\n",
    "        # if len(size) > 1:\n",
    "        #     n = size[1]\n",
    "        # return np.random.uniform(-1/math.sqrt(n), 1/math.sqrt(n), size)\n",
    "        return np.random.normal(0, 0.2, size)\n",
    "\n",
    "    @staticmethod\n",
    "    def biases_init(size):\n",
    "        return np.random.normal(0, 0.2, size)\n",
    "\n",
    "    # Because of overflow\n",
    "    # as explained here\n",
    "    # https://shaktiwadekar.medium.com/how-to-avoid-numerical-overflow-in-sigmoid-function-numerically-stable-sigmoid-function-5298b14720f6\n",
    "    @staticmethod\n",
    "    def sigmoid(x):\n",
    "        return np.where(x >= 0, (1 / (1 + np.exp(-x))), (np.exp(x) / (1 + np.exp(x))) )\n",
    "         # return 1 / (1 + np.exp(-x))\n",
    "\n",
    "    @staticmethod\n",
    "    def sigmoid_derivative(x):\n",
    "        return MiniBatchDeepNetwork.sigmoid(x) * (1 - MiniBatchDeepNetwork.sigmoid(x))\n",
    "\n",
    "    @staticmethod\n",
    "    def relu(x):\n",
    "        return max(0, x)\n",
    "\n",
    "    # Taken from https://stackoverflow.com/questions/46411180/implement-relu-derivative-in-python-numpy\n",
    "    @staticmethod\n",
    "    def relu_derivative(x):\n",
    "        return (x > 0) * 1\n",
    "\n",
    "    @staticmethod\n",
    "    def logistic_loss(a, y):\n",
    "        return - (y * math.log(a + MiniBatchDeepNetwork.epsilon) + (1 - y) * math.log(1 - a + MiniBatchDeepNetwork.epsilon))\n",
    "\n",
    "    # Proper transpose, not this numpy BS\n",
    "    @staticmethod\n",
    "    def transpose(x):\n",
    "        if len(np.shape(x)) == 1:\n",
    "            return x.reshape(1, -1)\n",
    "        return np.transpose(x)\n",
    "\n",
    "    @staticmethod\n",
    "    def one_d_always_two_d(x):\n",
    "        if len(np.shape(x)) == 1:\n",
    "            return x[:, np.newaxis]\n",
    "        else:\n",
    "            return x\n",
    "\n",
    "    # Based on https://en.wikipedia.org/wiki/Feature_scaling\n",
    "    @staticmethod\n",
    "    def normalize(x, x_min, x_max, new_min=-1, new_max=1):\n",
    "        return ((x - x_min) / (x_max - x_min)) * (new_max - new_min) + new_min\n",
    "\n",
    "    @staticmethod\n",
    "    def normalizer_constructor (data, normalize):\n",
    "        t_data = MiniBatchDeepNetwork.transpose(data)\n",
    "        x_min = []\n",
    "        x_max = []\n",
    "        for d_vector in t_data:\n",
    "            x_min.append(min(d_vector))\n",
    "            x_max.append(max(d_vector))\n",
    "\n",
    "        return lambda x: np.array([normalize(xx, x_min[idx], x_max[idx]) for idx, xx in enumerate(x)])\n",
    "\n",
    "    @staticmethod\n",
    "    def shuffle (data, labels):\n",
    "        # Below 3 lines based on https://play.pixelblaster.ro/blog/2017/01/20/how-to-shuffle-two-arrays-to-the-same-order/\n",
    "        l = labels.shape[0]\n",
    "        s = np.arange(l) # seed\n",
    "        np.random.shuffle(s)\n",
    "\n",
    "        data = np.array(data[s])\n",
    "        labels = np.array(labels[s])\n",
    "\n",
    "        return data, labels\n",
    "\n",
    "    def add_layer(self, k, weights_init = None,\n",
    "                 biases_init = None,\n",
    "                 activation = None,\n",
    "                 activation_derivative = None,\n",
    "                 input_size = None\n",
    "                 ):\n",
    "\n",
    "        if weights_init is None:\n",
    "            weights_init = MiniBatchDeepNetwork.weights_init\n",
    "\n",
    "        if biases_init is None:\n",
    "            biases_init = MiniBatchDeepNetwork.biases_init\n",
    "\n",
    "        if activation is None:\n",
    "            activation = MiniBatchDeepNetwork.sigmoid\n",
    "\n",
    "        if activation_derivative is None:\n",
    "            activation_derivative = MiniBatchDeepNetwork.sigmoid_derivative\n",
    "\n",
    "        if len(self.input_size) == 0 and input_size is None:\n",
    "            raise Exception(\"You are creating an input layer and did not provide input size\")\n",
    "        elif len(self.k) > 0:\n",
    "            input_size = self.k[-1]\n",
    "\n",
    "        self.k.append(k)\n",
    "        self.input_size.append(input_size)\n",
    "        self.weights.append(weights_init((k, input_size)))\n",
    "        self.biases.append(biases_init((k,)))\n",
    "        self.activation.append(np.vectorize(activation))\n",
    "        self.activation_derivative.append(np.vectorize(activation_derivative))\n",
    "\n",
    "    def train(self, data, labels, alpha=0.01, stop_early=False, stop_avg_loss=0.01, n_loss_avg=10,\n",
    "              mbatch_size = 10,\n",
    "              epochs = 5,\n",
    "              loss_function=None,\n",
    "              normalize=None\n",
    "              ):\n",
    "\n",
    "        if loss_function is None:\n",
    "            loss_function = MiniBatchDeepNetwork.logistic_loss\n",
    "\n",
    "        if normalize is None:\n",
    "            normalize = MiniBatchDeepNetwork.normalize\n",
    "\n",
    "        self.loss_function = np.vectorize(loss_function)\n",
    "        alpha = float(alpha)\n",
    "        self.normalizer = MiniBatchDeepNetwork.normalizer_constructor(data, normalize)\n",
    "\n",
    "        losses = []\n",
    "        for iter in range(epochs):\n",
    "            # randomize data in each epoch\n",
    "            data, labels = MiniBatchDeepNetwork.shuffle(data, labels)\n",
    "\n",
    "            # Split into batches\n",
    "            mbatches_data = np.array_split(data, mbatch_size)\n",
    "            mbatches_labels = np.array_split(labels, mbatch_size)\n",
    "\n",
    "            for mbatch_data, mbatch_labels in zip(mbatches_data, mbatches_labels):\n",
    "                losses = []\n",
    "                weights_sum = []\n",
    "                biases_sum = []\n",
    "                for weights, biases in zip(self.weights, self.biases):\n",
    "                    weights_sum.append(np.zeros(np.shape(weights)))\n",
    "                    biases_sum.append(np.zeros(np.shape(biases)))\n",
    "\n",
    "                for dp, label in zip(mbatch_data, mbatch_labels):\n",
    "                    a, z = self.f_propagate(dp)\n",
    "                    # loss = self.loss_function(a[-1], label)[0]\n",
    "                    # losses.append(loss)\n",
    "\n",
    "                    # Based on my old assignment https://github.com/jaroslawjanas/NN-Perceptron\n",
    "                    # (math is included in the repo https://github.com/jaroslawjanas/NN-Perceptron/tree/master/maths) which\n",
    "                    # is based on this playlist of videos https://youtube.com/playlist?list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi\n",
    "                    # and partially based on https://sudeepraja.github.io/Neural/\n",
    "\n",
    "                    dL_da_mn = None\n",
    "                    for i in range(1, len(self.k)):\n",
    "\n",
    "                        if(i == 1):\n",
    "                            # Output layer\n",
    "\n",
    "                            # dL_dz = a[-i] - label  # <-- This is faster because it's mathematically simplified\n",
    "                            # dL_da = (a[-i] - label) / ((a[-i] * (1 - a[-i])) + MiniBatchDeepNetwork.epsilon) # this is ugly\n",
    "\n",
    "                            # this avoids division by 0\n",
    "                            if (a[-i] * (1 - a[-i])) == 0:\n",
    "                                dL_da = (a[-i] - label) / MiniBatchDeepNetwork.epsilon\n",
    "                            else:\n",
    "                                dL_da = (a[-i] - label) / (a[-i] * (1 - a[-i]))\n",
    "\n",
    "                            da_dz = self.activation_derivative[-i](z[-i])\n",
    "                            dL_dz = dL_da *  da_dz # <-- but this is more flexible, I can specify my own activation for output layer\n",
    "                            dz_dw = a[-(i+1)]  # previous layer\n",
    "                            dz_db = 1\n",
    "                            dz_da_m1 = self.weights[-i]\n",
    "\n",
    "\n",
    "                            dL_dw = dL_dz.dot(MiniBatchDeepNetwork.transpose(dz_dw))\n",
    "                            dL_db = dL_dz * dz_db\n",
    "                            dL_da_mn = MiniBatchDeepNetwork.transpose(dz_da_m1).dot(dL_dz) # this line is based on https://sudeepraja.github.io/Neural/\n",
    "\n",
    "                            weights_sum[-i] -= alpha * dL_dw\n",
    "                            biases_sum[-i] -= alpha * dL_db\n",
    "\n",
    "                        else:\n",
    "                            # Hidden Layers\n",
    "\n",
    "                            da_m1_dz_m1 = self.activation_derivative[-i](z[-i])\n",
    "                            dL_dz_m1 = dL_da_mn * da_m1_dz_m1\n",
    "                            dz_m1_dw_m1 = a[-(i+1)]\n",
    "                            dz_m1_db_m1 = 1\n",
    "                            if i is not len(self.k):\n",
    "                                dz_m1_da_m2 = self.weights[-i]\n",
    "\n",
    "                            # dL_dw_m1 = dL_dz_m1[:,np.newaxis].dot(MiniBatchDeepNetwork.transpose(dz_m1_dw_m1))\n",
    "                            dL_dw_m1 = MiniBatchDeepNetwork.one_d_always_two_d(dL_dz_m1).dot(MiniBatchDeepNetwork.transpose(dz_m1_dw_m1))\n",
    "                            # above line is based on https://sudeepraja.github.io/Neural/\n",
    "                            dL_db_m1 = dL_dz_m1 * dz_m1_db_m1\n",
    "\n",
    "                            if i is not len(self.k):\n",
    "                                dL_da_mn = MiniBatchDeepNetwork.transpose(dz_m1_da_m2).dot(dL_dz_m1)\n",
    "\n",
    "                            weights_sum[-i] -= alpha * dL_dw_m1\n",
    "                            biases_sum[-i] -=  alpha * dL_db_m1\n",
    "\n",
    "                for weights, biases, w_sum, b_sum in zip(self.weights, self.biases, weights_sum, biases_sum):\n",
    "                    weights += w_sum / len(weights_sum)\n",
    "                    biases += b_sum / len(biases_sum)\n",
    "\n",
    "    def f_propagate(self, in_dp):\n",
    "        norm_in_dp = self.normalizer(in_dp)\n",
    "        a = [norm_in_dp]\n",
    "        zz = []\n",
    "        for weights, biases, activation in zip(self.weights, self.biases, self.activation):\n",
    "\n",
    "            z = np.dot(weights, a[-1]) + biases\n",
    "            zz = zz + [z]\n",
    "            a = a + [activation(z)]\n",
    "\n",
    "        return a, zz\n",
    "\n",
    "    def predict(self, in_dp):\n",
    "        a, _ = self.f_propagate(in_dp)\n",
    "        return a[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Blobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "minib_dnn_blobs = MiniBatchDeepNetwork()\n",
    "minib_dnn_blobs.add_layer(10, input_size=len(data[\"blobs\"][\"training\"][\"input\"][0]), activation=DeepNetwork.relu, activation_derivative=DeepNetwork.relu_derivative)\n",
    "minib_dnn_blobs.add_layer(1)\n",
    "\n",
    "minib_dnn_blobs.train(data[\"blobs\"][\"training\"][\"input\"], data[\"blobs\"][\"training\"][\"classes\"],\n",
    "                alpha=0.6, mbatch_size=15, epochs = 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 0.8444444444444444\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "for dp, label in zip(data[\"blobs\"][\"testing\"][\"input\"], data[\"blobs\"][\"testing\"][\"classes\"]):\n",
    "    a = minib_dnn_blobs.predict(dp)\n",
    "\n",
    "    if round(a[0]) == label:\n",
    "        correct += 1\n",
    "\n",
    "print(f\"Accuracy = {correct / len(data['blobs']['testing']['classes'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 0.7777777777777778\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "for dp, label in zip(data[\"blobs\"][\"validation\"][\"input\"], data[\"blobs\"][\"validation\"][\"classes\"]):\n",
    "    a = minib_dnn_blobs.predict(dp)\n",
    "\n",
    "    if round(a[0]) == label:\n",
    "        correct += 1\n",
    "\n",
    "print(f\"Accuracy = {correct / len(data['blobs']['validation']['classes'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Good results and quicker training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Circles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "minib_dnn_circles = MiniBatchDeepNetwork()\n",
    "minib_dnn_circles.add_layer(32, input_size=len(data[\"circles\"][\"training\"][\"input\"][0]), activation=DeepNetwork.relu, activation_derivative=DeepNetwork.relu_derivative)\n",
    "minib_dnn_circles.add_layer(16)\n",
    "minib_dnn_circles.add_layer(1)\n",
    "\n",
    "minib_dnn_circles.train(data[\"circles\"][\"training\"][\"input\"], data[\"circles\"][\"training\"][\"classes\"],\n",
    "                  alpha=0.04, mbatch_size=20, epochs = 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 0.9\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "for dp, label in zip(data[\"circles\"][\"testing\"][\"input\"], data[\"circles\"][\"testing\"][\"classes\"]):\n",
    "    a = minib_dnn_circles.predict(dp)\n",
    "\n",
    "    if round(a[0]) == label:\n",
    "        correct += 1\n",
    "\n",
    "print(f\"Accuracy = {correct / len(data['circles']['testing']['classes'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 0.9\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "for dp, label in zip(data[\"circles\"][\"validation\"][\"input\"], data[\"circles\"][\"validation\"][\"classes\"]):\n",
    "    a = minib_dnn_circles.predict(dp)\n",
    "\n",
    "    if round(a[0]) == label:\n",
    "        correct += 1\n",
    "\n",
    "print(f\"Accuracy = {correct / len(data['circles']['validation']['classes'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "The results continue to match or outperform those of a `DeepNetwork`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Horse Truck"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "minib_dnn_horse_truck = MiniBatchDeepNetwork()\n",
    "minib_dnn_horse_truck.add_layer(256, input_size=len(data[\"horse_truck\"][\"training\"][\"input\"][0]), activation=DeepNetwork.relu, activation_derivative=DeepNetwork.relu_derivative)\n",
    "minib_dnn_horse_truck.add_layer(64, activation=DeepNetwork.relu, activation_derivative=DeepNetwork.relu_derivative)\n",
    "minib_dnn_horse_truck.add_layer(32)\n",
    "minib_dnn_horse_truck.add_layer(1)\n",
    "\n",
    "minib_dnn_horse_truck.train(data[\"horse_truck\"][\"training\"][\"input\"], data[\"horse_truck\"][\"training\"][\"classes\"],\n",
    "                  # alpha=0.05, iterations=40_000,\n",
    "                  alpha=0.005, mbatch_size=32, epochs = 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 0.7449664429530202\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "for dp, label in zip(data[\"horse_truck\"][\"testing\"][\"input\"], data[\"horse_truck\"][\"testing\"][\"classes\"]):\n",
    "    a = minib_dnn_horse_truck.predict(dp)\n",
    "\n",
    "    if round(a[0]) == label:\n",
    "        correct += 1\n",
    "\n",
    "print(f\"Accuracy = {correct / len(data['horse_truck']['testing']['classes'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 0.7878787878787878\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "for dp, label in zip(data[\"horse_truck\"][\"validation\"][\"input\"], data[\"horse_truck\"][\"validation\"][\"classes\"]):\n",
    "    a = minib_dnn_horse_truck.predict(dp)\n",
    "\n",
    "    if round(a[0]) == label:\n",
    "        correct += 1\n",
    "\n",
    "print(f\"Accuracy = {correct / len(data['horse_truck']['validation']['classes'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "It took about the same time but yielded much more consistent results. It seems like convergence is much better."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "c529e9d9a92e9ffe906d7b8d4f145ac7732f0940b3bf2e0707363ba96b0f3885"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
